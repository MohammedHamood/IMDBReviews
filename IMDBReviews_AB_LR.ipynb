{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohammedHamood/IMDBReviews/blob/main/IMDBReviews_AB_LR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrsQVT2u4oiG"
      },
      "source": [
        "# IMDB Reviews - AdaBoost with Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5SxYRNM41Jn"
      },
      "source": [
        "## Data Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwUbdQkCmm6w",
        "outputId": "907cb00d-7c7d-4c67-8ebe-a9f36697776f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import preprocessingNLP as PNLP\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.decomposition import PCA, NMF, TruncatedSVD\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.datasets import load_files\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Import Dataset\n",
        "print(\"Downloading Dataset ...\")\n",
        "!wget -nv \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "IMDB_train = load_files('aclImdb/train/', categories=(\"pos\", \"neg\"), encoding='utf-8')\n",
        "IMDB_test = load_files('aclImdb/test/', categories=('pos', 'neg'), encoding='utf-8')\n",
        "print(\"Dataset Downloaded\")\n",
        "\n",
        "# Preprocessing\n",
        "print(\"PREPROCESSING ...\")\n",
        "IMDB_train.data = PNLP.customNLP(IMDB_train.data)\n",
        "IMDB_test.data = PNLP.customNLP(IMDB_test.data)\n",
        "IMDB_train.data, IMDB_train.target = PNLP.removeEmptyInstances(IMDB_train.data, IMDB_train.target)\n",
        "print(\"PREPROCESSING DONE!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Downloading Dataset ...\n",
            "2020-03-10 17:15:14 URL:http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz [84125825/84125825] -> \"aclImdb_v1.tar.gz\" [1]\n",
            "Dataset Downloaded\n",
            "PREPROCESSING ...\n",
            "PREPROCESSING DONE!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESL2_mmU5Bf4"
      },
      "source": [
        "## Setting Hyper-Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYhj5pMq5X59"
      },
      "source": [
        "In the following section, RandomSearchCV is used to evaluate the validation set accuracy of AdaBoost based on random combinations of hyper-parameters used in the validation pipeline. The relevant parameters needed are ngram_range for CountVectorizer and use_idf for TfidfTransformer. AdaBoostClassifier is used with its default attributes except for base_estimator, where LogisticRegression is applied with its optimal parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8CdHr-Qk3KR",
        "outputId": "7b0cad89-83b4-4cdf-9a96-59e951a9a1f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        }
      },
      "source": [
        "# Define parameters\n",
        "parameters = {\n",
        "     'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
        "     'tfidf__use_idf': (True, False),\n",
        "}\n",
        "\n",
        "# Create a pipeline\n",
        "pip = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('Norm', Normalizer(copy=False)),\n",
        "                ('clf', AdaBoostClassifier(base_estimator=LogisticRegression(\n",
        "                    max_iter=300, penalty='none', solver='newton-cg', tol=0.0001, n_jobs=-1)))])\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "n_iter_search = 6\n",
        "cv_folds = 3\n",
        "Ada_LR_rand_search = RandomizedSearchCV(pip, param_distributions=parameters, \n",
        "                               n_iter=n_iter_search, cv=cv_folds)\n",
        "\n",
        "# Utility function to report best scores\n",
        "def report(results, n_top=10):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n",
        "                  .format(results['mean_test_score'][candidate],\n",
        "                          results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"Mean Fit Time: %.3f seconds\" %results['mean_fit_time'][candidate])\n",
        "            print(\"\")\n",
        "\n",
        "# Execute RandomizedSearchCV and print best results\n",
        "Ada_LR_rand_search.fit(IMDB_train.data, IMDB_train.target)\n",
        "report(Ada_LR_rand_search.cv_results_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model with rank: 1\n",
            "Mean validation score: 0.890 (std: 0.003)\n",
            "Parameters: {'vect__ngram_range': (1, 2), 'tfidf__use_idf': True}\n",
            "Mean Fit Time: 10.873 seconds\n",
            "\n",
            "Model with rank: 2\n",
            "Mean validation score: 0.887 (std: 0.003)\n",
            "Parameters: {'vect__ngram_range': (1, 3), 'tfidf__use_idf': True}\n",
            "Mean Fit Time: 24.118 seconds\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.882 (std: 0.002)\n",
            "Parameters: {'vect__ngram_range': (1, 2), 'tfidf__use_idf': False}\n",
            "Mean Fit Time: 12.105 seconds\n",
            "\n",
            "Model with rank: 4\n",
            "Mean validation score: 0.882 (std: 0.002)\n",
            "Parameters: {'vect__ngram_range': (1, 3), 'tfidf__use_idf': False}\n",
            "Mean Fit Time: 26.471 seconds\n",
            "\n",
            "Model with rank: 5\n",
            "Mean validation score: 0.870 (std: 0.002)\n",
            "Parameters: {'vect__ngram_range': (1, 1), 'tfidf__use_idf': True}\n",
            "Mean Fit Time: 2.174 seconds\n",
            "\n",
            "Model with rank: 6\n",
            "Mean validation score: 0.855 (std: 0.002)\n",
            "Parameters: {'vect__ngram_range': (1, 1), 'tfidf__use_idf': False}\n",
            "Mean Fit Time: 2.555 seconds\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoJZo7t4tpDH"
      },
      "source": [
        "These results show a brief idea of the required parameters for CountVectorizer and TfidfTransformer.\n",
        "RandomSearchCV constantly gives different rankings as it always chooses its parameters randomly. However, setting *use_idf=TRUE* for TfidfTransformer clearly seems to increase the mean validation set accuracy. This confirms that downscaling weights for words that occur in many documents improves the probability of obtaining the best accuracy. Additionally, it is a good idea to evaluate a larger size of word n-grams for the model. *ngram_range=(1, 2)* will be used for CountVectorizer.\n",
        "\n",
        "Now, GridSearchCV is used to evaluate the hyper-parameters that could potentially optimize the validation accuracy of AdaBoostClassifier. Only the relevant parameters of the model are evaluated by combining different values for *n_estimators* and *learning_rate*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqJjV_t5-0-e",
        "outputId": "61a40ed8-8129-4cc0-cd1f-398fe685abae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        }
      },
      "source": [
        "# Set relevant parameters\n",
        "parameters = {\n",
        "    'n_estimators': (30, 50, 100, 200),\n",
        "    'learning_rate': (0.01, 0.1, 1)\n",
        "}\n",
        "\n",
        "# Create a pipeline\n",
        "pip = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
        "                ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                ('Norm', Normalizer(copy=False)),\n",
        "                ('clf', GridSearchCV(AdaBoostClassifier(\n",
        "                    base_estimator=LogisticRegression(\n",
        "                        max_iter=300, penalty='none', solver='newton-cg', tol=0.0001, n_jobs=-1)),\n",
        "                        parameters, cv=cv_folds, n_jobs=-1))])\n",
        "\n",
        "# Execute pipeline\n",
        "pip.fit(IMDB_train.data, IMDB_train.target)\n",
        "\n",
        "# Collect and print results\n",
        "test_accuracies = pip['clf'].cv_results_['mean_test_score']\n",
        "test_time = pip['clf'].cv_results_['mean_fit_time']\n",
        "test_params = pip['clf'].cv_results_['params']\n",
        "\n",
        "for i in range(len(test_time)):\n",
        "  print(\"Parameter: {0}\".format(test_params[i]))\n",
        "  print(\"Training Time: %.3f seconds\" %test_time[i])\n",
        "  print(\"Valdation Accuracy: {0:.3f}\".format(test_accuracies[i]))\n",
        "  print(\"\")\n",
        "\n",
        "print(\"Validation Accuracy: \" + str(pip['clf'].best_score_))\n",
        "print(\"Optimal Parameters: \" + str(pip['clf'].best_params_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8907998316572933\n",
            "Optimal Parameters: {'learning_rate': 0.01, 'n_estimators': 30}\n",
            "\n",
            "Parameter: {'learning_rate': 0.01, 'n_estimators': 30}\n",
            "Training Time: 4.697 seconds\n",
            "Valdation Accuracy: 0.891\n",
            "\n",
            "Parameter: {'learning_rate': 0.01, 'n_estimators': 50}\n",
            "Training Time: 4.582 seconds\n",
            "Valdation Accuracy: 0.891\n",
            "\n",
            "Parameter: {'learning_rate': 0.01, 'n_estimators': 100}\n",
            "Training Time: 4.550 seconds\n",
            "Valdation Accuracy: 0.891\n",
            "\n",
            "Parameter: {'learning_rate': 0.01, 'n_estimators': 200}\n",
            "Training Time: 4.549 seconds\n",
            "Valdation Accuracy: 0.891\n",
            "\n",
            "Parameter: {'learning_rate': 0.1, 'n_estimators': 30}\n",
            "Training Time: 4.587 seconds\n",
            "Valdation Accuracy: 0.891\n",
            "\n",
            "Parameter: {'learning_rate': 0.1, 'n_estimators': 50}\n",
            "Training Time: 4.551 seconds\n",
            "Valdation Accuracy: 0.891\n",
            "\n",
            "Parameter: {'learning_rate': 0.1, 'n_estimators': 100}\n",
            "Training Time: 4.579 seconds\n",
            "Valdation Accuracy: 0.891\n",
            "\n",
            "Parameter: {'learning_rate': 0.1, 'n_estimators': 200}\n",
            "Training Time: 4.620 seconds\n",
            "Valdation Accuracy: 0.891\n",
            "\n",
            "Parameter: {'learning_rate': 1, 'n_estimators': 30}\n",
            "Training Time: 4.552 seconds\n",
            "Valdation Accuracy: 0.891\n",
            "\n",
            "Parameter: {'learning_rate': 1, 'n_estimators': 50}\n",
            "Training Time: 4.618 seconds\n",
            "Valdation Accuracy: 0.891\n",
            "\n",
            "Parameter: {'learning_rate': 1, 'n_estimators': 100}\n",
            "Training Time: 4.755 seconds\n",
            "Valdation Accuracy: 0.891\n",
            "\n",
            "Parameter: {'learning_rate': 1, 'n_estimators': 200}\n",
            "Training Time: 4.711 seconds\n",
            "Valdation Accuracy: 0.891\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZZs95uxGOBu"
      },
      "source": [
        "Clearly, varying *learning_rate* and *n_estimators* does not have a significant impact on the validation set accuracy. Therefore, their default values will be used for calculating the test set accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X066MGWZ9gii"
      },
      "source": [
        "# Final Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi5JkcxcHNJm",
        "outputId": "da090e6f-1dcc-4acd-a7c9-c42c4dd72298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "# Create a pipeline\n",
        "pip = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
        "                ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                ('Norm', Normalizer(copy=False)),\n",
        "                ('clf', AdaBoostClassifier(base_estimator=LogisticRegression(\n",
        "                        max_iter=300, penalty='none', solver='newton-cg', tol=0.0001, n_jobs=-1)))])\n",
        "\n",
        "# Evaluate validation set accuracy\n",
        "start_time = time.time()\n",
        "scores = cross_val_score(pip, IMDB_train.data, IMDB_train.target, cv=10)\n",
        "valid_accuracy = np.mean(scores)\n",
        "print(\"10-Cross Validation Runtime: %s seconds\" % (time.time() - start_time))\n",
        "print(\"Validation Set Accuracy: {0}\".format(valid_accuracy))\n",
        "\n",
        "# Fit the model\n",
        "start_time = time.time()\n",
        "pip.fit(IMDB_train.data, IMDB_train.target)\n",
        "print(\"Training Runtime: %s seconds\" % (time.time() - start_time))\n",
        "\n",
        "# Get prediction on test set\n",
        "start_time = time.time()\n",
        "IMDB_pred = pip.predict(IMDB_test.data)\n",
        "print(\"Prediction Runtime: %s seconds\" % (time.time() - start_time))\n",
        "\n",
        "# Compute test set accuracy\n",
        "test_accuracy = np.mean(IMDB_pred==IMDB_test.target)\n",
        "print(\"Test Set Accuracy: {0}\".format(test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10-Cross Validation Runtime: 221.6829879283905 seconds\n",
            "Validation Set Accuracy: 0.89612\n",
            "Training Runtime: 22.44027042388916 seconds\n",
            "Prediction Runtime: 7.357857704162598 seconds\n",
            "Test Set Accuracy: 0.88464\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}